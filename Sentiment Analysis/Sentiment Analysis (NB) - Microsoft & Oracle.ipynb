{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5195423",
   "metadata": {},
   "source": [
    "# MICROSOFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7745f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Necessary Modules\n",
    "import nltk\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from emot.emo_unicode import UNICODE_EMOJI # For emojis\n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b84db10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dec-2022</td>\n",
       "      <td>SOCIALIZED MEDIA: HUNDREDS OF ISRAEL’S UNIT 82...</td>\n",
       "      <td>Tibou33969029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dec-2022</td>\n",
       "      <td>RT HuffPostWomen: The Microsoft mogul gave $5 ...</td>\n",
       "      <td>GrowGirlathon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dec-2022</td>\n",
       "      <td>The Microsoft mogul gave $5 billion to the Bil...</td>\n",
       "      <td>LinusAlso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dec-2022</td>\n",
       "      <td>Microsoft SQL/C Developer at RBC\\nCome Work wi...</td>\n",
       "      <td>zobjobsCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dec-2022</td>\n",
       "      <td>Large NYC taxi fleet looking to hire a SECRETA...</td>\n",
       "      <td>radio_rusrek</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Datetime                                               Text       Username\n",
       "0  Dec-2022  SOCIALIZED MEDIA: HUNDREDS OF ISRAEL’S UNIT 82...  Tibou33969029\n",
       "1  Dec-2022  RT HuffPostWomen: The Microsoft mogul gave $5 ...  GrowGirlathon\n",
       "2  Dec-2022  The Microsoft mogul gave $5 billion to the Bil...      LinusAlso\n",
       "3  Dec-2022  Microsoft SQL/C Developer at RBC\\nCome Work wi...      zobjobsCA\n",
       "4  Dec-2022  Large NYC taxi fleet looking to hire a SECRETA...   radio_rusrek"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "Microsoft_df = pd.read_csv('Dataset/Companies/microsoft.csv')\n",
    "Microsoft_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35dfbf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2953 entries, 0 to 2952\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Datetime  2953 non-null   object\n",
      " 1   Text      2953 non-null   object\n",
      " 2   Username  2953 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 69.3+ KB\n"
     ]
    }
   ],
   "source": [
    "Microsoft_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80bdb2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2937 entries, 0 to 2952\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Datetime  2937 non-null   object\n",
      " 1   Text      2937 non-null   object\n",
      " 2   Username  2937 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 91.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#Drop Duplicate Rows\n",
    "Microsoft_df = Microsoft_df.drop_duplicates()\n",
    "Microsoft_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94c9884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter Data by Different Time Periods\n",
    "Microsoft_Dec22 = Microsoft_df[Microsoft_df['Datetime'] == 'Dec-2022']\n",
    "Microsoft_Jan23 = Microsoft_df[Microsoft_df['Datetime'] == 'Jan-2023']\n",
    "Microsoft_Feb23 = Microsoft_df[Microsoft_df['Datetime'] == 'Feb-2023']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f8dc1",
   "metadata": {},
   "source": [
    "# Tweets PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198d2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URLs\n",
    "Microsoft_Dec22 = Microsoft_Dec22['Text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "Microsoft_Jan23 = Microsoft_Jan23['Text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "Microsoft_Feb23 = Microsoft_Feb23['Text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f19b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove HTML Reference Characters\n",
    "Microsoft_Dec22 = Microsoft_Dec22.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
    "Microsoft_Jan23 = Microsoft_Jan23.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
    "Microsoft_Feb23 = Microsoft_Feb23.apply(lambda x: re.sub(r'&[a-z]+;', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68dbf474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Twitter Handles\n",
    "Microsoft_Dec22 = Microsoft_Dec22.apply(lambda x: re.sub(r'@[^ ]+', '', x))\n",
    "Microsoft_Jan23 = Microsoft_Jan23.apply(lambda x: re.sub(r'@[^ ]+', '', x))\n",
    "Microsoft_Feb23 = Microsoft_Feb23.apply(lambda x: re.sub(r'@[^ ]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a385cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Emojis with Words\n",
    "with open('Libraries/Emoji_Dict.p', 'rb') as fp:\n",
    "    Emoji_Dict = pickle.load(fp)\n",
    "Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "\n",
    "def convert_emojis_to_word(text):\n",
    "    for emot in Emoji_Dict:\n",
    "        text = re.sub(r'('+emot+')', \"_\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "Microsoft_Dec22 = Microsoft_Dec22.apply(convert_emojis_to_word)\n",
    "Microsoft_Jan23 = Microsoft_Jan23.apply(convert_emojis_to_word)\n",
    "Microsoft_Feb23 = Microsoft_Feb23.apply(convert_emojis_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67f19d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace EMOTICONS with Words\n",
    "with open('Libraries/Emoticon_Dict.p', 'rb') as fp:\n",
    "    Emoticon_Dict = pickle.load(fp)\n",
    "\n",
    "def convert_emoticons(text):\n",
    "    for emot in Emoticon_Dict:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(Emoticon_Dict[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "Microsoft_Dec22 = Microsoft_Dec22.apply(convert_emoticons)\n",
    "Microsoft_Jan23 = Microsoft_Jan23.apply(convert_emoticons)\n",
    "Microsoft_Feb23 = Microsoft_Feb23.apply(convert_emoticons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127aea7",
   "metadata": {},
   "source": [
    "# Perform Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08bf1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Training Dataset\n",
    "training_dataset = pd.read_csv('Dataset/Training/training_dataset.csv', encoding='latin-1')\n",
    "\n",
    "#Create Training Data\n",
    "train_text = []\n",
    "for index,row in training_dataset.iterrows():\n",
    "    train_text.append((row['Text'], row['Label']))\n",
    "\n",
    "#Feature Extraction Function\n",
    "def extract_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    return dict((word, True) for word in words)\n",
    "\n",
    "#Create Feature Sets from Training Data\n",
    "train_features = [(extract_features(text), label) for (text, label) in train_text]\n",
    "\n",
    "#Train the Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "#For Microsoft December 2022\n",
    "test_df1 = Microsoft_Dec22.to_frame()\n",
    "for index,row in test_df1.iterrows():\n",
    "    test_features = extract_features(row['Text'])\n",
    "    predicted_label = classifier.classify(test_features)\n",
    "    test_df1.at[index,'sentiment'] = predicted_label\n",
    "    \n",
    "#For Microsoft January 2023\n",
    "test_df2 = Microsoft_Jan23.to_frame()\n",
    "for index,row in test_df2.iterrows():\n",
    "    test_features = extract_features(row['Text'])\n",
    "    predicted_label = classifier.classify(test_features)\n",
    "    test_df2.at[index,'sentiment'] = predicted_label\n",
    "    \n",
    "#For Microsoft Februrary 2023\n",
    "test_df3 = Microsoft_Feb23.to_frame()\n",
    "for index,row in test_df3.iterrows():\n",
    "    test_features = extract_features(row['Text'])\n",
    "    predicted_label = classifier.classify(test_features)\n",
    "    test_df3.at[index,'sentiment'] = predicted_label\n",
    "\n",
    "#Export Sentiments Predicted Dataset\n",
    "test_df1.to_csv(\"Dataset/Sentiments Predicted/MicrosoftDec22_Sentiments.csv\")\n",
    "test_df2.to_csv(\"Dataset/Sentiments Predicted/MicrosoftJan23_Sentiments.csv\")\n",
    "test_df3.to_csv(\"Dataset/Sentiments Predicted/MicrosoftFeb23_Sentiments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e578254",
   "metadata": {},
   "source": [
    "# Analysis of Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a300732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft December 2022\n",
      "Positive Labels: 0.24163179916317992\n",
      "Negative Labels: 0.694560669456067\n",
      "Neutral Labels: 0.06380753138075314\n",
      "--------------------\n",
      "Microsoft January 2023\n",
      "Positive Labels: 0.22121212121212122\n",
      "Negative Labels: 0.7070707070707071\n",
      "Neutral Labels: 0.07171717171717172\n",
      "--------------------\n",
      "Microsoft February 2023\n",
      "Positive Labels: 0.26639757820383453\n",
      "Negative Labels: 0.693239152371342\n",
      "Neutral Labels: 0.04036326942482341\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "Microsoft_Dec22 = pd.read_csv(\"Dataset/Sentiments Predicted/MicrosoftDec22_Sentiments.csv\", encoding='latin-1')\n",
    "Microsoft_Jan23 = pd.read_csv(\"Dataset/Sentiments Predicted/MicrosoftJan23_Sentiments.csv\", encoding='latin-1')\n",
    "Microsoft_Feb23 = pd.read_csv(\"Dataset/Sentiments Predicted/MicrosoftFeb23_Sentiments.csv\", encoding='latin-1')\n",
    "\n",
    "#Used to Store the Labels & Total Number of Tweets\n",
    "labels_Microsoft_Dec22 = []\n",
    "count_Microsoft_Dec22 = 0\n",
    "labels_Microsoft_Jan23 = []\n",
    "count_Microsoft_Jan23 = 0\n",
    "labels_Microsoft_Feb23 = []\n",
    "count_Microsoft_Feb23 = 0\n",
    "\n",
    "#For Microsoft December 2022\n",
    "for index,row in Microsoft_Dec22.iterrows():\n",
    "    labels_Microsoft_Dec22.append(row['sentiment'])\n",
    "    count_Microsoft_Dec22 += 1\n",
    "\n",
    "#Count the Number of Positive, Negative and Neutral Labels\n",
    "num_positives = labels_Microsoft_Dec22.count('pos')\n",
    "num_negatives = labels_Microsoft_Dec22.count('neg')\n",
    "num_neutral = labels_Microsoft_Dec22.count('neu')\n",
    "\n",
    "#Print the Results\n",
    "print('Microsoft December 2022')\n",
    "print('Positive Labels:', num_positives/count_Microsoft_Dec22)\n",
    "print('Negative Labels:', num_negatives/count_Microsoft_Dec22)\n",
    "print('Neutral Labels:', num_neutral/count_Microsoft_Dec22)\n",
    "\n",
    "\n",
    "#For Microsoft January 2023\n",
    "for index,row in Microsoft_Jan23.iterrows():\n",
    "    labels_Microsoft_Jan23.append(row['sentiment'])\n",
    "    count_Microsoft_Jan23 += 1\n",
    "\n",
    "#Count the Number of Positive, Negative and Neutral Labels\n",
    "num_positives = labels_Microsoft_Jan23.count('pos')\n",
    "num_negatives = labels_Microsoft_Jan23.count('neg')\n",
    "num_neutral = labels_Microsoft_Jan23.count('neu')\n",
    "\n",
    "#Print the Results\n",
    "print('--------------------')\n",
    "print('Microsoft January 2023')\n",
    "print('Positive Labels:', num_positives/count_Microsoft_Jan23)\n",
    "print('Negative Labels:', num_negatives/count_Microsoft_Jan23)\n",
    "print('Neutral Labels:', num_neutral/count_Microsoft_Jan23)\n",
    "\n",
    "#For Microsoft February 2023\n",
    "for index,row in Microsoft_Feb23.iterrows():\n",
    "    labels_Microsoft_Feb23.append(row['sentiment'])\n",
    "    count_Microsoft_Feb23 += 1\n",
    "\n",
    "#Count the Number of Positive, Negative and Neutral Labels\n",
    "num_positives = labels_Microsoft_Feb23.count('pos')\n",
    "num_negatives = labels_Microsoft_Feb23.count('neg')\n",
    "num_neutral = labels_Microsoft_Feb23.count('neu')\n",
    "\n",
    "#Print the Results\n",
    "print('--------------------')\n",
    "print('Microsoft February 2023')\n",
    "print('Positive Labels:', num_positives/count_Microsoft_Feb23)\n",
    "print('Negative Labels:', num_negatives/count_Microsoft_Feb23)\n",
    "print('Neutral Labels:', num_neutral/count_Microsoft_Feb23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530a71d",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification - Calculate Accuracy of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a24f7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Training Dataset\n",
    "training_dataset = pd.read_csv('Dataset/Training/training_dataset.csv', encoding='latin-1')\n",
    "\n",
    "#Create Training Data\n",
    "train_text = []\n",
    "for index,row in training_dataset.iterrows():\n",
    "    train_text.append((row['Text'], row['Label']))\n",
    "\n",
    "#Feature Extraction Function\n",
    "def extract_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    return dict((word, True) for word in words)\n",
    "\n",
    "#Create Feature Sets from Training Data\n",
    "train_features = [(extract_features(text), label) for (text, label) in train_text]\n",
    "\n",
    "#Train the Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "#Test on Testing Dataset\n",
    "test_df = pd.read_csv('Dataset/Comparing Accuracy/microsoft_goldtruth_cleaned.csv', encoding='latin-1')\n",
    "for index,row in test_df.iterrows():\n",
    "    test_features = extract_features(row['Text'])\n",
    "    predicted_label = classifier.classify(test_features)\n",
    "    test_df.at[index,'predicted sentiment'] = predicted_label\n",
    "\n",
    "#Export Sentiments Predicted Dataset\n",
    "test_df.to_csv(\"Dataset/Comparing Accuracy/Microsoft_NBClassifier_CompareAccuracy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9774b7dc",
   "metadata": {},
   "source": [
    "# Calculate Accuracy of Naive Bayes Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "589b7e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "#Load dataset\n",
    "df = pd.read_csv('Dataset/Comparing Accuracy/Microsoft_NBClassifier_CompareAccuracy.csv', encoding='latin-1')\n",
    "\n",
    "#Calculate Accuracy\n",
    "microsoft_accuracycount = 0\n",
    "for i in range(len(df)):\n",
    "    if df['goldtruth'][i] == df['predicted sentiment'][i]:\n",
    "        microsoft_accuracycount += 1\n",
    "        \n",
    "print(microsoft_accuracycount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c55146",
   "metadata": {},
   "source": [
    "# TP/TN/FP/FN Calculation - Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fb03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#33 negative\n",
    "#36 neutral \n",
    "#31 positive in goldtruth of microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b38dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "55\n",
      "14\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "## for the 'positive' class, count number of TP, TN, FP and Fn\n",
    "\n",
    "tp_microsoft_pos = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'pos' and df['goldtruth'][i] == 'pos':\n",
    "        tp_microsoft_pos += 1\n",
    "        \n",
    "        \n",
    "tn_microsoft_pos = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'pos' and df['goldtruth'][i] != 'pos':\n",
    "        tn_microsoft_pos += 1\n",
    "        \n",
    "fp_microsoft_pos = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'pos' and df['goldtruth'][i] != 'pos':\n",
    "        fp_microsoft_pos += 1\n",
    "        \n",
    "fn_microsoft_pos = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'pos' and df['goldtruth'][i] == 'pos':\n",
    "        fn_microsoft_pos += 1\n",
    "        \n",
    "print(tp_microsoft_pos)\n",
    "print(tn_microsoft_pos)\n",
    "print(fp_microsoft_pos)\n",
    "print(fn_microsoft_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de52bbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "21\n",
      "46\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "## for the 'negative' class, count number of TP, TN, FP and Fn\n",
    "\n",
    "tp_microsoft_neg = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'neg' and df['goldtruth'][i] == 'neg':\n",
    "        tp_microsoft_neg += 1\n",
    "        \n",
    "        \n",
    "tn_microsoft_neg = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'neg' and df['goldtruth'][i] != 'neg':\n",
    "        tn_microsoft_neg += 1\n",
    "        \n",
    "fp_microsoft_neg = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'neg' and df['goldtruth'][i] != 'neg':\n",
    "        fp_microsoft_neg += 1\n",
    "        \n",
    "fn_microsoft_neg = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'neg' and df['goldtruth'][i] == 'neg':\n",
    "        fn_microsoft_neg += 1\n",
    "        \n",
    "print(tp_microsoft_neg)\n",
    "print(tn_microsoft_neg)\n",
    "print(fp_microsoft_neg)\n",
    "print(fn_microsoft_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "425f932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "61\n",
      "3\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "## for the 'neutral' class, count number of TP, TN, FP and Fn\n",
    "\n",
    "tp_microsoft_neu = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'neu' and df['goldtruth'][i] == 'neu':\n",
    "        tp_microsoft_neu += 1\n",
    "        \n",
    "        \n",
    "tn_microsoft_neu = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'neu' and df['goldtruth'][i] != 'neu':\n",
    "        tn_microsoft_neu += 1\n",
    "        \n",
    "fp_microsoft_neu = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'neu' and df['goldtruth'][i] != 'neu':\n",
    "        fp_microsoft_neu += 1\n",
    "        \n",
    "fn_microsoft_neu = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'neu' and df['goldtruth'][i] == 'neu':\n",
    "        fn_microsoft_neu += 1\n",
    "        \n",
    "print(tp_microsoft_neu)\n",
    "print(tn_microsoft_neu)\n",
    "print(fp_microsoft_neu)\n",
    "print(fn_microsoft_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096bf41",
   "metadata": {},
   "source": [
    "# ORACLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db9242aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dec-2022</td>\n",
       "      <td>”Drywall's \"Work The Dumb Oracle\" album's warm...</td>\n",
       "      <td>Stan_Ridgway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dec-2022</td>\n",
       "      <td>@ORACLE_ECHO @snakeeyes828 @CPD1617Scanner Wha...</td>\n",
       "      <td>GrizzledTexan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dec-2022</td>\n",
       "      <td>My copy of FANTASTIC FRIGHTS arrived, marking ...</td>\n",
       "      <td>lomakescomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dec-2022</td>\n",
       "      <td>Look at any university in the US, they have cl...</td>\n",
       "      <td>NullRSJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dec-2022</td>\n",
       "      <td>@MsJoyceTarot Ms Joyce, I’ve followed your wor...</td>\n",
       "      <td>KLVNKBRWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Datetime                                               Text       Username\n",
       "0  Dec-2022  ”Drywall's \"Work The Dumb Oracle\" album's warm...   Stan_Ridgway\n",
       "1  Dec-2022  @ORACLE_ECHO @snakeeyes828 @CPD1617Scanner Wha...  GrizzledTexan\n",
       "2  Dec-2022  My copy of FANTASTIC FRIGHTS arrived, marking ...  lomakescomics\n",
       "3  Dec-2022  Look at any university in the US, they have cl...        NullRSJ\n",
       "4  Dec-2022  @MsJoyceTarot Ms Joyce, I’ve followed your wor...      KLVNKBRWN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Dataset\n",
    "Oracle_df = pd.read_csv('Dataset/Companies/oracle.csv')\n",
    "Oracle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7461dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2842 entries, 0 to 2841\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Datetime  2842 non-null   object\n",
      " 1   Text      2842 non-null   object\n",
      " 2   Username  2842 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 66.7+ KB\n"
     ]
    }
   ],
   "source": [
    "Oracle_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44087ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2780 entries, 0 to 2841\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Datetime  2780 non-null   object\n",
      " 1   Text      2780 non-null   object\n",
      " 2   Username  2780 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 86.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#Drop Duplicate Rows\n",
    "Oracle_df = Oracle_df.drop_duplicates()\n",
    "Oracle_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b502da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter Data by Different Time Periods\n",
    "Oracle_Dec22 = Oracle_df[Oracle_df['Datetime'] == 'Dec-2022']\n",
    "Oracle_Jan23 = Oracle_df[Oracle_df['Datetime'] == 'Jan-2023']\n",
    "Oracle_Feb23 = Oracle_df[Oracle_df['Datetime'] == 'Feb-2023']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b324447",
   "metadata": {},
   "source": [
    "# Tweets PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "337f6460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URLs\n",
    "Oracle_Dec22 = Oracle_Dec22['Text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "Oracle_Jan23 = Oracle_Jan23['Text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "Oracle_Feb23 = Oracle_Feb23['Text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19b83db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove HTML Reference Characters\n",
    "Oracle_Dec22 = Oracle_Dec22.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
    "Oracle_Jan23 = Oracle_Jan23.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
    "Oracle_Feb23 = Oracle_Feb23.apply(lambda x: re.sub(r'&[a-z]+;', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "113ca3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Twitter Handles\n",
    "Oracle_Dec22 = Oracle_Dec22.apply(lambda x: re.sub(r'@[^ ]+', '', x))\n",
    "Oracle_Jan23 = Oracle_Jan23.apply(lambda x: re.sub(r'@[^ ]+', '', x))\n",
    "Oracle_Feb23 = Oracle_Feb23.apply(lambda x: re.sub(r'@[^ ]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15c0c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Emojis with Words\n",
    "with open('Libraries/Emoji_Dict.p', 'rb') as fp:\n",
    "    Emoji_Dict = pickle.load(fp)\n",
    "Emoji_Dict = {v: k for k, v in Emoji_Dict.items()}\n",
    "\n",
    "def convert_emojis_to_word(text):\n",
    "    for emot in Emoji_Dict:\n",
    "        text = re.sub(r'('+emot+')', \"_\".join(Emoji_Dict[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "Oracle_Dec22 = Oracle_Dec22.apply(convert_emojis_to_word)\n",
    "Oracle_Jan23 = Oracle_Jan23.apply(convert_emojis_to_word)\n",
    "Oracle_Feb23 = Oracle_Feb23.apply(convert_emojis_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d0fd9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace EMOTICONS with Words\n",
    "with open('Libraries/Emoticon_Dict.p', 'rb') as fp:\n",
    "    Emoticon_Dict = pickle.load(fp)\n",
    "\n",
    "def convert_emoticons(text):\n",
    "    for emot in Emoticon_Dict:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(Emoticon_Dict[emot].replace(\",\",\"\").split()), text)\n",
    "    return text\n",
    "\n",
    "Oracle_Dec22 = Oracle_Dec22.apply(convert_emoticons)\n",
    "Oracle_Jan23 = Oracle_Jan23.apply(convert_emoticons)\n",
    "Oracle_Feb23 = Oracle_Feb23.apply(convert_emoticons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118d55bd",
   "metadata": {},
   "source": [
    "# Perform Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44624768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Training Dataset\n",
    "training_dataset = pd.read_csv('Dataset/Training/training_dataset.csv', encoding='latin-1')\n",
    "\n",
    "#Create Training Data\n",
    "train_text = []\n",
    "for index,row in training_dataset.iterrows():\n",
    "    train_text.append((row['Text'], row['Label']))\n",
    "\n",
    "#Feature Extraction Function\n",
    "def extract_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    return dict((word, True) for word in words)\n",
    "\n",
    "#Create Feature Sets from Training Data\n",
    "train_features = [(extract_features(text), label) for (text, label) in train_text]\n",
    "\n",
    "#Train the Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "#For Oracle December 2022\n",
    "test_df1 = Oracle_Dec22.to_frame()\n",
    "for index,row in test_df1.iterrows():\n",
    "    test_features = extract_features(row['Text'])\n",
    "    predicted_label = classifier.classify(test_features)\n",
    "    test_df1.at[index,'sentiment'] = predicted_label\n",
    "    \n",
    "#For Oracle January 2023\n",
    "test_df2 = Oracle_Jan23.to_frame()\n",
    "for index,row in test_df2.iterrows():\n",
    "    test_features = extract_features(row['Text'])\n",
    "    predicted_label = classifier.classify(test_features)\n",
    "    test_df2.at[index,'sentiment'] = predicted_label\n",
    "    \n",
    "#For Oracle Februrary 2023\n",
    "test_df3 = Oracle_Feb23.to_frame()\n",
    "for index,row in test_df3.iterrows():\n",
    "    test_features = extract_features(row['Text'])\n",
    "    predicted_label = classifier.classify(test_features)\n",
    "    test_df3.at[index,'sentiment'] = predicted_label\n",
    "\n",
    "#Export Sentiments Predicted Dataset\n",
    "test_df1.to_csv(\"Dataset/Sentiments Predicted/OracleDec22_Sentiments.csv\")\n",
    "test_df2.to_csv(\"Dataset/Sentiments Predicted/OracleJan23_Sentiments.csv\")\n",
    "test_df3.to_csv(\"Dataset/Sentiments Predicted/OracleFeb23_Sentiments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9dfd8",
   "metadata": {},
   "source": [
    "# Analysis of Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44b767fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle December 2022\n",
      "Positive Labels: 0.4957983193277311\n",
      "Negative Labels: 0.35084033613445376\n",
      "Neutral Labels: 0.15336134453781514\n",
      "--------------------\n",
      "Oracle January 2023\n",
      "Positive Labels: 0.5264976958525346\n",
      "Negative Labels: 0.3352534562211982\n",
      "Neutral Labels: 0.1382488479262673\n",
      "--------------------\n",
      "Oracle February 2023\n",
      "Positive Labels: 0.48541666666666666\n",
      "Negative Labels: 0.371875\n",
      "Neutral Labels: 0.14270833333333333\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "Oracle_Dec22 = pd.read_csv(\"Dataset/Sentiments Predicted/OracleDec22_Sentiments.csv\", encoding='latin-1')\n",
    "Oracle_Jan23 = pd.read_csv(\"Dataset/Sentiments Predicted/OracleJan23_Sentiments.csv\", encoding='latin-1')\n",
    "Oracle_Feb23 = pd.read_csv(\"Dataset/Sentiments Predicted/OracleFeb23_Sentiments.csv\", encoding='latin-1')\n",
    "\n",
    "#Used to Store the Labels & Total Number of Tweets\n",
    "labels_Oracle_Dec22 = []\n",
    "count_Oracle_Dec22 = 0\n",
    "labels_Oracle_Jan23 = []\n",
    "count_Oracle_Jan23 = 0\n",
    "labels_Oracle_Feb23 = []\n",
    "count_Oracle_Feb23 = 0\n",
    "\n",
    "#For Oracle December 2022\n",
    "for index,row in Oracle_Dec22.iterrows():\n",
    "    labels_Oracle_Dec22.append(row['sentiment'])\n",
    "    count_Oracle_Dec22 += 1\n",
    "\n",
    "#Count the Number of Positive, Negative and Neutral Labels\n",
    "num_positives = labels_Oracle_Dec22.count('pos')\n",
    "num_negatives = labels_Oracle_Dec22.count('neg')\n",
    "num_neutral = labels_Oracle_Dec22.count('neu')\n",
    "\n",
    "#Print the Results\n",
    "print('Oracle December 2022')\n",
    "print('Positive Labels:', num_positives/count_Oracle_Dec22)\n",
    "print('Negative Labels:', num_negatives/count_Oracle_Dec22)\n",
    "print('Neutral Labels:', num_neutral/count_Oracle_Dec22)\n",
    "\n",
    "\n",
    "#For Oracle January 2023\n",
    "for index,row in Oracle_Jan23.iterrows():\n",
    "    labels_Oracle_Jan23.append(row['sentiment'])\n",
    "    count_Oracle_Jan23 += 1\n",
    "\n",
    "#Count the Number of Positive, Negative and Neutral Labels\n",
    "num_positives = labels_Oracle_Jan23.count('pos')\n",
    "num_negatives = labels_Oracle_Jan23.count('neg')\n",
    "num_neutral = labels_Oracle_Jan23.count('neu')\n",
    "\n",
    "#Print the Results\n",
    "print('--------------------')\n",
    "print('Oracle January 2023')\n",
    "print('Positive Labels:', num_positives/count_Oracle_Jan23)\n",
    "print('Negative Labels:', num_negatives/count_Oracle_Jan23)\n",
    "print('Neutral Labels:', num_neutral/count_Oracle_Jan23)\n",
    "\n",
    "#For Oracle February 2023\n",
    "for index,row in Oracle_Feb23.iterrows():\n",
    "    labels_Oracle_Feb23.append(row['sentiment'])\n",
    "    count_Oracle_Feb23 += 1\n",
    "\n",
    "#Count the Number of Positive, Negative and Neutral Labels\n",
    "num_positives = labels_Oracle_Feb23.count('pos')\n",
    "num_negatives = labels_Oracle_Feb23.count('neg')\n",
    "num_neutral = labels_Oracle_Feb23.count('neu')\n",
    "\n",
    "#Print the Results\n",
    "print('--------------------')\n",
    "print('Oracle February 2023')\n",
    "print('Positive Labels:', num_positives/count_Oracle_Feb23)\n",
    "print('Negative Labels:', num_negatives/count_Oracle_Feb23)\n",
    "print('Neutral Labels:', num_neutral/count_Oracle_Feb23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6f343",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification - Calculate Accuracy of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f1a47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Training Dataset\n",
    "training_dataset = pd.read_csv('Dataset/Training/training_dataset.csv', encoding='latin-1')\n",
    "\n",
    "#Create Training Data\n",
    "train_text = []\n",
    "for index,row in training_dataset.iterrows():\n",
    "    train_text.append((row['Text'], row['Label']))\n",
    "\n",
    "#Feature Extraction Function\n",
    "def extract_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    return dict((word, True) for word in words)\n",
    "\n",
    "#Create Feature Sets from Training Data\n",
    "train_features = [(extract_features(text), label) for (text, label) in train_text]\n",
    "\n",
    "#Train the Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "#Test on Testing Dataset\n",
    "test_df = pd.read_csv('Dataset/Comparing Accuracy/oracle_goldtruth_cleaned.csv', encoding='latin-1')\n",
    "for index,row in test_df.iterrows():\n",
    "    test_features = extract_features(row['Text'])\n",
    "    predicted_label = classifier.classify(test_features)\n",
    "    test_df.at[index,'predicted sentiment'] = predicted_label\n",
    "\n",
    "#Export Sentiments Predicted Dataset\n",
    "test_df.to_csv(\"Dataset/Comparing Accuracy/Oracle_NBClassifier_CompareAccuracy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e05818",
   "metadata": {},
   "source": [
    "# Calculate Accuracy of Naive Bayes Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "676a67fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "#Load dataset\n",
    "df = pd.read_csv('Dataset/Comparing Accuracy/Oracle_NBClassifier_CompareAccuracy.csv', encoding='latin-1')\n",
    "\n",
    "#Calculate Accuracy\n",
    "oracle_accuracycount = 0\n",
    "for i in range(len(df)):\n",
    "    if df['goldtruth'][i] == df['predicted sentiment'][i]:\n",
    "        oracle_accuracycount += 1\n",
    "        \n",
    "print(oracle_accuracycount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1143e0",
   "metadata": {},
   "source": [
    "# Calculate Overall Accuracy of Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea8db7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43\n"
     ]
    }
   ],
   "source": [
    "overall_accuracy = (oracle_accuracycount + microsoft_accuracycount) / 200\n",
    "print(overall_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17298cb0",
   "metadata": {},
   "source": [
    "# TP/TN/FP/FN Calculation - Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa68da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#28 negative\n",
    "#43 neutral\n",
    "#29 positive \n",
    "# true labels in oracle goldtruth \n",
    "\n",
    "# adding up total labels across microsoft/oracle \n",
    "# negative : 28 + 33 =61\n",
    "\n",
    "# neutral : 43 + 36 = 79\n",
    "\n",
    "# positive: 29+ 31 = 60\n",
    "\n",
    "# not balanced dataset so thats why need weighted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4bd3822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "45\n",
      "26\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "## for the 'positive' class, count number of TP, TN, FP and Fn\n",
    "\n",
    "tp_oracle_pos = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'pos' and df['goldtruth'][i] == 'pos':\n",
    "        tp_oracle_pos += 1\n",
    "        \n",
    "        \n",
    "tn_oracle_pos = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'pos' and df['goldtruth'][i] != 'pos':\n",
    "        tn_oracle_pos += 1\n",
    "        \n",
    "fp_oracle_pos = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'pos' and df['goldtruth'][i] != 'pos':\n",
    "        fp_oracle_pos += 1\n",
    "        \n",
    "fn_oracle_pos = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'pos' and df['goldtruth'][i] == 'pos':\n",
    "        fn_oracle_pos += 1\n",
    "        \n",
    "print(tp_oracle_pos)\n",
    "print(tn_oracle_pos)\n",
    "print(fp_oracle_pos)\n",
    "print(fn_oracle_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "323b19b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "47\n",
      "25\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "## for the 'negative' class, count number of TP, TN, FP and Fn\n",
    "\n",
    "tp_oracle_neg = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'neg' and df['goldtruth'][i] == 'neg':\n",
    "        tp_oracle_neg += 1\n",
    "        \n",
    "        \n",
    "tn_oracle_neg = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'neg' and df['goldtruth'][i] != 'neg':\n",
    "        tn_oracle_neg += 1\n",
    "        \n",
    "fp_oracle_neg = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'neg' and df['goldtruth'][i] != 'neg':\n",
    "        fp_oracle_neg += 1\n",
    "        \n",
    "fn_oracle_neg = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'neg' and df['goldtruth'][i] == 'neg':\n",
    "        fn_oracle_neg += 1\n",
    "        \n",
    "print(tp_oracle_neg)\n",
    "print(tn_oracle_neg)\n",
    "print(fp_oracle_neg)\n",
    "print(fn_oracle_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7ae2d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "57\n",
      "0\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "## for the 'neutral' class, count number of TP, TN, FP and Fn\n",
    "\n",
    "tp_oracle_neu = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'neu' and df['goldtruth'][i] == 'neu':\n",
    "        tp_oracle_neu += 1\n",
    "        \n",
    "        \n",
    "tn_oracle_neu = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'neu' and df['goldtruth'][i] != 'neu':\n",
    "        tn_oracle_neu += 1\n",
    "        \n",
    "fp_oracle_neu = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] == 'neu' and df['goldtruth'][i] != 'neu':\n",
    "        fp_oracle_neu += 1\n",
    "        \n",
    "fn_oracle_neu = 0\n",
    "for i in range(len(df)):\n",
    "    if df['predicted sentiment'][i] != 'neu' and df['goldtruth'][i] == 'neu':\n",
    "        fn_oracle_neu += 1\n",
    "        \n",
    "print(tp_oracle_neu)\n",
    "print(tn_oracle_neu)\n",
    "print(fp_oracle_neu)\n",
    "print(fn_oracle_neu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31830c18",
   "metadata": {},
   "source": [
    "# Weighted Precision/Recall/F1 Score Computation - Microsoft & Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eecb7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive class\n",
    "tp_pos = tp_oracle_pos + tp_microsoft_neg\n",
    "tn_pos = tn_oracle_pos + tn_microsoft_neg\n",
    "fp_pos = fp_oracle_pos + fp_microsoft_neg\n",
    "fn_pos = fn_oracle_pos + fn_microsoft_neg\n",
    "\n",
    "#negative class\n",
    "\n",
    "tp_neg = tp_oracle_neg + tp_microsoft_neg\n",
    "tn_neg = tn_oracle_neg + tn_microsoft_neg\n",
    "fp_neg = fp_oracle_neg + fp_microsoft_neg\n",
    "fn_neg = fn_oracle_neg + fn_microsoft_neg\n",
    "\n",
    "#neutral class\n",
    "\n",
    "tp_neu = tp_oracle_neu + tp_microsoft_neu\n",
    "tn_neu = tn_oracle_neu + tn_microsoft_neu\n",
    "fp_neu = fp_oracle_neu + fp_microsoft_neu\n",
    "fn_neu = fn_oracle_neu + fn_microsoft_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5228952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive class\n",
    "\n",
    "precision_pos = tp_pos / (tp_pos + fp_pos)\n",
    "recall_pos = tp_pos / (tp_pos + fn_pos)\n",
    "\n",
    "# negative class\n",
    "\n",
    "precision_neg = tp_neg / (tp_neg + fp_neg)\n",
    "recall_neg = tp_neg / (tp_neg + fn_neg)\n",
    "\n",
    "# neutral class\n",
    "\n",
    "precision_neu = tp_neu / (tp_neu + fp_neu)\n",
    "recall_neu = tp_neu / (tp_neu + fn_neu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c476fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights for each class since we did not have a labelled dataset \n",
    "\n",
    "w_Positive = 200 / (3 * 60)\n",
    "\n",
    "w_Negative = 200 / (3 * 61)\n",
    "\n",
    "w_Neutral = 200 / (3 * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08459b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Macroaveraged_Precision = (w_Positive*precision_pos + w_Negative*precision_neg + w_Neutral * precision_neu) / 3\n",
    "\n",
    "Macroaveraged_Recall = (w_Positive*recall_pos + w_Negative*recall_neg + w_Neutral*recall_neu) / 3\n",
    "\n",
    "f1 = 2 * ( ( (Macroaveraged_Precision) * (Macroaveraged_Recall) ) / ( (Macroaveraged_Precision) + (Macroaveraged_Recall) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23fe5399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macroaveraged_Precision: 0.5019924402384506\n",
      "Macroaveraged_Recall: 0.5934951473585859\n",
      "F1 Score: 0.5439223240232746\n"
     ]
    }
   ],
   "source": [
    "print('Macroaveraged_Precision:',Macroaveraged_Precision)\n",
    "print('Macroaveraged_Recall:',Macroaveraged_Recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d9b4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
